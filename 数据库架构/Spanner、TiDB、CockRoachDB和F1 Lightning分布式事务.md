# 目录

[toc]

# 分布式事务的目标

## ACID

数据库事务的目标是实现ACID，即原子性、一致性、隔离性、持久性。

- Atomic原子性: 一个事务的所有系列操作步骤被看成是一个动作，所有的步骤要么全部完成要么一个也不会完成，如果事务过程中任何一点失败，将要被改变的数据库记录就不会被真正被改变。
- Consistent一致性: 数据库的约束级联和触发机制Trigger都必须满足事务的一致性。也就是说，通过各种途径包括外键约束等任何写入数据库的数据都是有效的，不能发生表与表之间存在外键约束，但是有数据却违背这种约束性。所有改变数据库数据的动作事务必须完成，没有事务会创建一个无效数据状态，这是不同于CAP理论的一致性"consistency".
- Isolated隔离性: 主要用于实现并发控制, 隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。
- Durable持久性: 一旦一个事务被提交，它应该持久保存，不会因为和其他操作冲突而取消这个事务。很多人认为这意味着事务是持久在磁盘上，但是规范没有特别定义这点。

针对单机数据库，A（原子性）、C（一致性）是比较容易满足的。基于undo log满足A，基于redo log、加锁满足C，但对于分布式数据库却比较困难，不止是从单机到多机复杂度提升，中间还有“网络”这个不稳定的元素。

## CAP

CAP 理论三个特性的详细含义如下：

1. 一致性（Consistency）：每次读取要么是最新的数据，要么是一个错误；
2. 可用性（Availability）：client 在任何时刻的读写操作都能在限定的延迟内完成的，即每次请求都能获得一个响应（非错误），但不保证是最新的数据；
3. 分区容忍性（Partition tolerance）：在大规模分布式系统中，网络分区现象，即分区间的机器无法进行网络通信的情况是必然会发生的，系统应该能保证在这种情况下可以正常工作。

<img src="https://matt33.com/images/distribute/CAP.png" alt="CAP 理论原理" style="zoom:50%;" />

## 挑战1：原子性

事务需要满足A：要么全部失败，要么全部成功，对于单机数据库，只要将事务写入日志，就能够保证此事务可以全部回滚或者可以全部提交，而分布式事务是多机事务，每个机器的情况不同，`有的机器可能写日志成功，有的机器可能写日志失败（比如因为获取锁失败），也就是会出现部分成功部分失败的情况，因此分布式数据库该如何保证原子性，是一个挑战`。

## 挑战2：事务顺序

后开始的事务要能够读到在它开始前已经结束的事务的写，为了正确的MVCC，事务需要有单调递增（Monotonically Increasing）的时间戳或者事务序号保证逻辑顺序。

在单机数据库要保证单调递增不是个问题，但是分布式数据库多点的，要如何保证为冲突事务分配的序号是单调递增就是个问题。

## 挑战3：不稳定的网络

单机的通讯是稳定的，一个操作无论失败还是成功，调用者都知道自己能够在未来的某个点得到确定的回复，而分布式数据库则只能通过网络通讯，网络是不稳定的，可能对方收不到你的指令，或者你收不到对方的回复，该怎么保证分布式事务的可用性，也就是在用户能够容忍的时间内得到确定的答复。

# 四种数据库实现的效果及思路

## 外部一致性（External Consistency）

对于client来说，它观察到的两个事务的顺序一定与它们的提交顺序相同，因此如果事务1在事务2之前commit，那么事务2一定能够读取到事务1的变更。

![image-20210301093851335](C:\Users\97168\AppData\Roaming\Typora\typora-user-images\image-20210301093851335.png)

虽然事务2在事务1提交前就已开始，但是它的commit更晚，一定能够读到事务1的变更。

### Spanner

Spanner 最广为人知的就是它使用了原子钟进行授时。但实际上原子钟只是手段，真正有开创意义的是 TrueTime。利用各种硬件设备（大多数情况下主要起作用的其实是 GPS）和算法，TrueTime API 可以对外返回当前估算时间及误差范围，事务逻辑在考虑到误差之后进行一些补偿，最后就能实现外部一致性了。

原理其实比较简单，打个比方说明下：你跟妹子约会，商量好了 12 点整在电影院门口碰头，结果你等到 12 点还没见到人。这时候你是不会直接离开的，因为你会想可能是两人表的时间没对准，在妹子看来还没到 12 点。然后你一直等到 12:30，发现妹子还没来，你就知道自己是被放鸽子了，毕竟表不准也不太可能差这么多。更进一步，假如你能精确知道误差范围，譬如说误差不超过 10 分钟，那么你等到 12:10 就能知道肯定等不来人了。

Spanner 的事务正是这么做的，核心点就是事务提交的时候，会等待误差范围那么长的时间，然后才给客户端返回。这样一来，客户端接着再启动事务，或者客户端用某种方式通知另一个机房的客户启动事务（即使使用量子通信），新事务的取到的时间一定会比前面那个事务提交的时间要晚。

### TiDB

TiDB 的方法是非常简单粗暴的，所有事务的 ts （用于标识事务的顺序）都要从中心节点 PD 获取，PD 在分配时保证 ts 严格单调递增。

因为所有事务都要通过同一个 PD 取 ts，假如在外部观察到两个事务有先后次序（如前所说，前一个事务提交完成后，第二个才启动），那么后面事务的 ts 一定会更大。于是，我们用 ts 的大小来规定事务的顺序，一定不会违背系统外部观察到的现象。

实际情况还要更复杂一点，因为事务往往涉及到多个节点，还需要使用 2PC 才能真正保证一致性，这里不展开了。

### CockroachDB

CockroachDB 用的是 HLC（混合逻辑时钟），使用的是结合物理时钟和逻辑时钟的时间戳。这个其实是权衡之后的方案：主打场景是类似 Spanner 那样的全球化部署，但是作为开源方案，也不可能用专有设备来搞一套 TrueTime。如果退而求其次用 NTP 的话，时钟误差无法控制下来会很大程度上影响性能。

HLC 是怎么一回事呢？简单理解下，就是两个事务如果时间间隔大的话，用物理时间能进行定序，如果物理时间上很接近，就依靠逻辑时钟来进行排序。

逻辑时钟的工作方式是，每当两个节点进行通信（比如收发消息包，本质上是产生信息交换时），都进行一次时钟同步。比如 A 给 B 发消息，带上自己的时钟 100，B 收到消息后发现自己的时钟是 80，此时 B 就会把自己本地的时钟设为 101，以此来保证不同节点事件时间戳的大小能体现因果关系。

我们用前面举的例子分析一下。

第一个事务先在节点 A 写，完成后第二个事务去节点 B 读同一份数据，如果 B 是可读的，那一定意味着在这之前 A 和 B 之间发生过信息交换（可以想象一下 paxos 或者 raft），而信息交换会触发逻辑时钟同步，这样第二个事务在 B 所取到的时间戳一定大于第一个事务在 A 取到的时间戳，于是外部一致性就得到保证了。

不过 HLC 也不是绝对安全的。对于前面说的那个 3 事务的例子，CockroachDB 就可能破坏一致性。究其原因，两个写事务写的数据是没有交集的，因此它们对应的两个节点之间没有进行时钟同步，于是有可能后面的事务取到的时间戳更小。这个其实是逻辑时钟最主要的缺陷：逻辑时钟依赖于追踪系统内事件的因果关系，如果因果关系不在系统内部，就无能为力了。

## 原子性

Spanner、TiDB、CockroachDB都使用的2pc保证原子性。

2pc 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为**协调者**的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

2PC协议用于保证属于多个数据分片上的操作的原子性。这些数据分片可能分布在不同的服务器上，2PC协议保证多台服务器上的操作要么全部成功，要么全部失败。

## 副本强一致性

### Spanner

![image-20210301104318624](https://gitee.com/yyjjtt/picture_bed/raw/master/img/20210301104339.png)

Spanner使用Paxos协议保证同一个数据分片的多个副本之间的数据一致性。当这些副本分布到不同的数据中心时，这个需求尤其强烈。

对于每个扮演领导者角色的副本，每个 spanserver 也会实施一个事务管理器来支持分布式事务。这个事务管理器被用来实现一个 participant leader，该组内的其他副本则是作为 participant slaves。如果一个事务只包含一个 Paxos 组(对于许多事务而言都是如此)，它就可以绕过事务管理器，因为锁表和 Paxos 二者一起可以保证事务性。如果一个事务包含了多 于一个 Paxos 组，那些组的领导者之间会彼此协调合作完成两阶段提交。其中一个参与者组，会被选为协调者，该组的 participant leader 被称为 coordinator leader，该组的 participant slaves 被称为 coordinator slaves。每个事务管理器的状态，会被保存到底层的 Paxos 组。目前 TiKV 的事务好像都走的是两阶段提交，不知到要是也搞个事务管理器效果会如何？

### TiDB

TiDB使用Raft协议保证副本数据一致性。

以TiKV为例，TiKV内部可分成多个模块，Raft模块，RocksDB模块，两者通过Log交互。整体架构图下图所示，consensus就是Raft模块，state machine就是RocksDB模块。

![图例 4](https://download.pingcap.com/images/blog-cn/linearizability-and-raft/4.jpg)

如图所示，Raft保证副本数据一致性的流程：

1. client发送请求
2. Leader将请求作为一个Proposal通过Raft复制到自身以及Follower的Log中，然后将其commit
3. commit成功后，TiKV将Log应用到RoaksDB上
4. 将结果返回给client

由于input（即Log）都一样，则TiKV的状态机的状态能达成一致。

### CockroachDB

和TiDB相同，使用raft保证副本数据一致

# 参考

[理解Google Spanner(3)：分布式事务原理与实现](https://juejin.cn/post/6844903998655299591#heading-5)

[分布式系统的一致性协议之 2PC 和 3PC | Matt's Blog](https://matt33.com/2018/07/08/distribute-system-consistency-protocol/)

[数据库的外部一致性](https://disksing.com/external-consistency/)

[Spanner 论文笔记 | Life is magic. Coding is art.](https://int64.me/2017/Spanner%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html)