# 目录

[toc]

# 参考文献

李航.统计学习方法2012第4章(MN有课程笔记)

[李航《统计学习方法》啃书指导~人美声甜的数学系博士小姐姐带你一起学习《机器学习》~_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili](https://www.bilibili.com/video/BV1i4411G7Xv?p=4)

# 原理

[条件概率，全概率，贝叶斯公式理解](<https://www.jianshu.com/p/c59851b1c0f3>)

## 简介

## 所属机器学习类型

生成方法

![image-20200930160900600](https://gitee.com//yyjjtt/picture_bed/raw/master/img/20200930160907.png)

## 生成模型

![image-20200930161957584](https://gitee.com//yyjjtt/picture_bed/raw/master/img/20200930161957.png)

## 条件概率公式

所谓"条件概率"（Conditional probability），就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。

P(AB)=P(A)P(B|A)=P(B)P(A|B)

## 全概率公式

由于后面要用到，所以除了条件概率以外，这里还要推导全概率公式。

假定样本空间S，是两个事件A与A'的和。

 

![img](https://upload-images.jianshu.io/upload_images/2127249-1c63915d1cb91d48.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/432/format/webp)

上图中，红色部分是事件A，绿色部分是事件A'，它们共同构成了样本空间S。

在这种情况下，事件B可以划分成两个部分。

 

![img](https://upload-images.jianshu.io/upload_images/2127249-4b27b8fe4bf7373a.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/432/format/webp)
$$
P(B)=P(B \cap A)+P\left(B \cap A^{\prime}\right)
$$

$$
P(B \cap A)=P(B | A) P(A)
$$

$$
P(B)=P(B | A) P(A)+P\left(B | A^{\prime}\right) P\left(A^{\prime}\right)
$$

这就是全概率公式。它的含义是，如果A和A'构成样本空间的一个划分，那么事件B的概率，就等于A和A'的概率分别乘以B对这两个事件的条件概率之和。

将这个公式代入上一节的条件概率公式，就得到了条件概率的另一种写法

##贝叶斯准则

由条件概率推出
$$
P(A | B) P(B)=P(B | A) P(A)
$$
贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法：
$$
p(c | x)=\frac{p(x | c) p(c)}{p(x)}
$$

## 先验概率和后验概率

在贝叶斯法则中，每个名词都有约定俗成的名称：

Pr(A)是A的**先验概率**或边缘概率。之所以称为"先验"是因为它不考虑任何B方面的因素。

Pr(A|B)是已知B发生后A的条件概率，也由于得自B的取值而被称作A的**后验概率**。

Pr(B|A)是已知A发生后B的条件概率，也由于得自A的取值而被称作B的**后验概率**。

Pr(B)是B的先验概率或边缘概率，也作**标准化常量（normalized constant）**。

按这些术语，Bayes法则可表述为：

后验概率 = (似然度 * 先验概率)/标准化常量 也就是说，后验概率与先验概率和似然度的乘积成正比。

另外，比例Pr(B|A)/Pr(B)也有时被称作标准似然度（standardised likelihood），Bayes法则可表述为：

后验概率 = 标准似然度 * 先验概率

 

##朴素

我们假设特征之间 **相互独立** 。所谓 **独立(independence)** 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 **朴素**(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，**每个特征同等重要**。

**Note:** 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。

## 贝叶斯推断的含义

对条件概率公式进行变形，可以得到如下形式
$$
P(A | B)=P(A) \frac{P(B | A)}{P(B)}
$$
我们把**P(A)称为"先验概率"**（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。

**P(A|B)称为"后验概率"（Posterior probability）**，即在B事件发生之后，我们对A事件概率的重新评估。**P(B|A)/P(B)称为"可能性函数"（Likelyhood）**，这是一个调整因子，使得预估概率更接近真实概率。

##  后验概率最大化

[朴素贝叶斯 后验概率最大化的含义_zhengjihao的博客-CSDN博客](https://blog.csdn.net/zhengjihao/article/details/78064121)

# 应用场景

## 文档分类

工作原理:

```
提取所有文档中的词条并进行去重
获取文档的所有类别
计算每个类别中的文档数目
对每篇训练文档:
    对每个类别:
        如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）
        增加所有词条的计数值（此类别下词条总数）
对每个类别:
    对每个词条:
        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）
返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）
```

开发流程

```
收集数据: 可以使用任何方法。
准备数据: 需要数值型或者布尔型数据。
分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
训练算法: 计算不同的独立特征的条件概率。
测试算法: 计算错误率。
使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本
```

朴素贝叶斯 算法特点

```
优点: 在数据较少的情况下仍然有效，可以处理多类别问题。
缺点: 对于输入数据的准备方式较为敏感。
适用数据类型: 标称型数据。
```

#实战

## 屏蔽社区留言板的侮辱性言论

### 项目概述

构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。

### 算法训练

现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 **w**. 粗体的 **w** 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。
$$
p\left(c_{l} | w\right)=\frac{p\left(w | c_{i}\right) p\left(c_{i}\right)}{p(w)}
$$
我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。

问: 上述代码实现中，为什么没有计算P(w)？

答：根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，**P(w)是固定的**(为啥是固定的,因为公式**不涉及c**)。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边**分子值**得大小来做决策分类。

p(ci) :首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。

p(w | ci) :这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个**独立特征**，那么就可以将上述概率写作

p(w0, w1, w2...wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。

`p(w0, w1, w2...wn | ci) -->p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci)`

 在案例中 p(w0|ci)  在侮辱性言论**w0出现的个数**/侮辱性言论**总的词数**

###数据优化

**分子分母为0**:在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数**初始化为1**，并将**分母初始化为 2** （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。

**下溢出:** 这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 **ln(a * b) = ln(a) + ln(b)**, 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。

下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。

![å½æ°å¾å](https://github.com/apachecn/AiLearning/raw/master/img/ml/4.NaiveBayesian/NB_7.png)

### code

bayes.py

```python
import numpy as np
def loadDataSet():
    """
    创建数据集
    :return: 单词列表postingList, 所属类别classVec
    """
    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], #[0,0,1,1,1......]
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not
    return postingList, classVec
#提取所有文档中的词条并进行去重
def createVocabList(dataSet):
    vocabSet=set([])#creat empty set
    for document in dataSet:
        #求两个集合的并集
        vocabSet=vocabSet|set(document)
    return list(vocabSet)
#遍历查看该单词是否出现，出现将单词置为1
def setOfWords2Vec(vocabList,inputSet):
    #创建一个和词汇表等长的向量 并将其元素置为0
    returnVec =[0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]=1
        else:
            print("This word{ } is not in my Vocabulary ".format(word))
    return returnVec
def trainNB0(trainMatrix,trainCategory):
    """
        训练数据原版
        :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...]
        :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵
        :return:
        """
    #文件数
    numTrainDocs=len(trainMatrix)
    #单词数
    numWords=len(trainMatrix[0])
    #侮辱性文件出现的概率 p(ci)
    pAbusive=sum(trainCategory)/float(numTrainDocs)
    #构造单词出现的次数列表
    p0Num=np.ones(numWords)
    p1Num=np.ones(numWords)
    p0Denom=2.0
    p1Denom=2.0
    for i in range(numTrainDocs):
        if trainCategory[i]==1:#侮辱性文件
            p1Num+=trainMatrix[i]#[0,1,1,....] + [0,1,1,....]->[0,2,2,...]
            p1Denom+=sum(trainMatrix[i])
        else:
            p0Num+=trainMatrix[i]
            p0Denom+=sum(trainMatrix[i])
    #求p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci)
    p1Vect=np.log(p1Num/p1Denom)# [1,2,3,5]/90->[1/90,...]
    p0Vect=np.log(p0Num/p0Denom)
    return p0Vect,p1Vect,pAbusive
 
def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):
    """
        使用算法：
            # 将乘法转换为加法
            乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)
            加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))
        :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量
        :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表
        :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表
        :param pClass1: 类别1，侮辱性文件的出现概率
        :return: 类别1 or 0
        """
    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))
    # 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，
    # 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。
    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。
    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来
    p1=sum(vec2Classify*p1Vec)+np.log(pClass1)
    p0=sum(vec2Classify*p0Vec)+np.log(1.0-pClass1)
    if p1>p0:
        return 1
    else:
        return 0
def testingNB():
    listOPosts,listClasses=loadDataSet()
    myVocabList=createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))
    # 训练数据
    p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(listClasses))
    #测试数据
    # 5. 测试数据
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))
    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))
 
    testEntry = ['stupid', 'garbage']
    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))
    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))
 
 
 
```

test

```python
import bayes
 
if __name__ == '__main__':
    listOPosts, listClasses = bayes.loadDataSet()
    myVocabList = bayes.createVocabList(listOPosts)
    print(bayes.setOfWords2Vec(myVocabList, listOPosts[0]))
    bayes.testingNB()
```

## 案例2:使用朴素贝叶斯过滤垃圾邮件

使用案例1的接口,只需要整理一下数据计科

```python
import bayes
import numpy as np
import random
# 切分文本
def textParse(bigString):
    '''
    Desc:
        接收一个大字符串并将其解析为字符串列表
    Args:
        bigString -- 大字符串
    Returns:
        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表
    '''
    import re
    # 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串 \W匹配非单词字符
    listOfTokens = re.split(r'\W*', bigString)
    return [tok.lower() for tok in listOfTokens if len(tok) > 2]
#主函数
def spamTest():
    docList=[]
    classList=[]
    fullTest=[]
    for i in range(1,26):
        #切分解析数据 并归类为1
        f=open("data/email/spam/%d.txt"%i,'r').read()
        wordList=textParse(f)
        docList.append(wordList)
        classList.append(1)
        f2 = open("data/email/ham/%d.txt"%i,'r').read()
        wordList=textParse(f2)
        docList.append(wordList)
        classList.append(0)
    #创建词汇表
    vocabList=bayes.createVocabList(docList)
    trainingSet=list(range(50))
    testSet=[]
    #随机取10个邮件来测试
    for i in range(10):
        print(len(trainingSet))
        randIndex=random.randint(0,len(trainingSet)-1)
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[]
    trainClasses=[]
    for docIndex in trainingSet:
        trainMat.append(bayes.setOfWords2Vec(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0Vect, p1Vect, pAbusive=bayes.trainNB0(np.array(trainMat),np.array(trainClasses))
    if 32 not in testSet:
        testSet.append(32)
    for i in testSet:
        test_Vec=bayes.setOfWords2Vec(vocabList,docList[i])
        #print(type(test_doc),type(p0Vect),type(p1Vect),type(pAbusive))
 
        res=bayes.classifyNB(np.array(test_Vec),p0Vect, p1Vect, pAbusive)
        if res==1:
            print("句子:{} 为1 实际:{}".format(i,classList[i]))
        else:
            print("句子:{}判定为0 实际:{}".format(i,classList[i]))
```

## 案例3使用朴素贝叶斯分类器从个人广告中获取区域倾向

<https://www.cnblogs.com/femaleprogramer/p/3854970.html>

###前置

我们将每个词的出现与否作为一个特征，这可以被描述为 **词集模型(set-of-words model)**。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 **词袋模型(bag-of-words model)**。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。

### code

```python
#词袋
def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0] * len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec
```

```python
import bayes
import feedparser
import random
import numpy as np
#RSS源分类器及高频词去除函数
def calcMostFreq(vocabList,fullText):
    import operator
    freqDict={}
    for token in vocabList:
        freqDict[token] =fullText.count(token)#统计每个词在文本中出现的个数
    sortedFreq =sorted(freqDict.items(),key=operator.itemgetter(1),reverse=True)
    return sortedFreq[:30]#返回出现次数最多的30个词
def localWords(feed1,feed0):
    docList=[]; classList=[];fullText=[]
    minLen=min(len(feed1['entries']),len(feed0['entries']))
    print(minLen)
    for i in range(minLen):
        wordList=bayes.textParse(feed1['entries'][i]['summary'])
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)
        wordList=bayes.textParse(feed0['entries'][i]['summary'])
        docList.append(wordList)
        fullText.append(wordList)
        classList.append(0)
    vocabList=bayes.createVocabList(docList)
    top30Words=calcMostFreq(vocabList,fullText)
    for pairW in top30Words:
        if pairW[0] in vocabList:
            vocabList.remove(pairW[0])#去掉出现次数最高的那些词
    trainSet=list(range(2*minLen))
    testSet=[]
    for i in range(3):
        print(len(trainSet)-1)
        randIndex=random.randint(0,len(trainSet)-1)
        testSet.append(trainSet[randIndex])
        del (trainSet[randIndex])
    trainMat=[];trainClasses=[]
    for docIndex in trainSet:
        trainMat.append(bayes.bagOfWords2VecMN(vocabList,docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0V,p1V,pSpam=bayes.trainNB0(np.array(trainMat),np.array(trainClasses))
    errorCount=0
    for docIndex in testSet:
        wordVector=bayes.bagOfWords2VecMN(vocabList,docList[docIndex])
        if bayes.classifyNB(np.array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:
            errorCount+=1
    print("the error rate is {}".format(float(errorCount)/len(testSet)))
    return vocabList,p0V,p1V
 
if __name__ == '__main__':
    nasa=feedparser.parse('http://www.nasa.gov/rss/dyn/image_of_the_day.rss')
    yahoosports=feedparser.parse('http://sports.yahoo.com/nba/teams/hou/rss.xml')
    #calcMostFreq()
    localWords(nasa,yahoosports)
```